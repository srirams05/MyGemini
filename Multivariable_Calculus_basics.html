<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Multivariable Calculus basics</title>
  <link rel="stylesheet" href="styles.css">
  </script> </head><script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>  
<body>
  <nav>
    <ul>
    <li><a href="#coordinatize-euclidean-space"
    id="toc-coordinatize-euclidean-space">Coordinatize Euclidean
    space</a></li>
    <li><a href="#partial-derivative"
    id="toc-partial-derivative">Partial Derivative</a></li>
    <li><a href="#frechet-derivative"
    id="toc-frechet-derivative">Frechet Derivative</a></li>
    <li><a href="#derivations-and-tangent-vectors"
    id="toc-derivations-and-tangent-vectors">Derivations and tangent
    vectors</a></li>
    <li><a href="#tangent-space-and-basis"
    id="toc-tangent-space-and-basis">Tangent space and basis</a></li>
    <li><a href="#tangent-space-via-curves"
    id="toc-tangent-space-via-curves">Tangent space via curves</a></li>
    <li><a href="#equivalence-of-tangent-space-definitions"
    id="toc-equivalence-of-tangent-space-definitions">Equivalence of
    tangent space definitions</a></li>
    </ul>
  </nav>
  <article>
    <h1 id="coordinatize-euclidean-space">Coordinatize Euclidean
    space</h1>
    <p><strong>Question on differential geometry. Consider the Euclidean
    space <span class="math inline">\(R^n\)</span>. In order to
    cooridinatize this space, I do the following. Let <span
    class="math inline">\(x = (x_1, \cdots, x_n)\)</span> where <span
    class="math inline">\(x_i: R^ \to  R\)</span> are the coordinate
    functions. For an arbitrary point <span class="math inline">\(p \in
    R^n\)</span>, <span class="math inline">\(x(p) \in R^n\)</span> is a
    vector of numbers which are coordinates of this point <span
    class="math inline">\(p\)</span>. Is this correct?</strong></p>
    <p>Yes, your description is correct.</p>
    <p>In Euclidean space <span
    class="math inline">\(\mathbb{R}^n\)</span>, we can define a
    coordinate system using <span class="math inline">\(n\)</span>
    coordinate functions <span class="math inline">\(x_i : \mathbb{R}^n
    \to \mathbb{R}\)</span>, where <span class="math inline">\(i = 1,
    \dots, n\)</span>. For a point <span class="math inline">\(p \in
    \mathbb{R}^n\)</span>, the vector <span class="math inline">\(x(p) =
    (x_1(p), x_2(p), \dots, x_n(p)) \in \mathbb{R}^n\)</span> represents
    the coordinates of <span class="math inline">\(p\)</span> in this
    coordinate system. Each <span class="math inline">\(x_i(p)\)</span>
    gives the <span class="math inline">\(i\)</span>-th coordinate of
    the point <span class="math inline">\(p\)</span>. This is a standard
    way to coordinatize <span
    class="math inline">\(\mathbb{R}^n\)</span>. The functions <span
    class="math inline">\(x_i\)</span> are typically chosen to be linear
    functions of the Cartesian coordinates. For example, in the standard
    Cartesian coordinate system, <span
    class="math inline">\(x_i(p)\)</span> is simply the <span
    class="math inline">\(i\)</span>-th component of the vector
    representing <span class="math inline">\(p\)</span> from the
    origin.</p>
    <p><strong>Maybe it is better to refer to the Euclidean space as
    <span class="math inline">\(E^n\)</span> to make it distinct from
    the vector space <span class="math inline">\(R^n\)</span> where the
    coordinate vectors live. What do you think?</strong></p>
    <p>You’re right to point out the subtle distinction. While often
    used interchangeably, <span class="math inline">\(E^n\)</span> and
    <span class="math inline">\(\mathbb{R}^n\)</span> represent slightly
    different mathematical objects:</p>
    <ul>
    <li><p><strong><span
    class="math inline">\(\mathbb{R}^n\)</span></strong>: This typically
    refers to the <em>vector space</em> of <span
    class="math inline">\(n\)</span>-tuples of real numbers. It’s a set
    with a defined vector addition and scalar multiplication.</p></li>
    <li><p><strong><span class="math inline">\(E^n\)</span></strong>:
    This often denotes <em>Euclidean n-space</em>, which is <span
    class="math inline">\(\mathbb{R}^n\)</span> equipped with the
    standard Euclidean inner product (dot product). This addition of the
    inner product gives <span class="math inline">\(E^n\)</span> a
    notion of distance and angle, which are not inherent in the vector
    space <span class="math inline">\(\mathbb{R}^n\)</span>
    alone.</p></li>
    </ul>
    <p>Therefore, using <span class="math inline">\(E^n\)</span> to
    explicitly refer to Euclidean space, distinct from the underlying
    vector space <span class="math inline">\(\mathbb{R}^n\)</span>, is a
    good practice, especially in differential geometry where the metric
    (distance) structure is crucial. It improves clarity and reduces
    potential ambiguity. The coordinates of a <em>point</em> in <span
    class="math inline">\(E^n\)</span> are then elements of the vector
    space <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
    <h1 id="partial-derivative">Partial Derivative</h1>
    <p><strong>Now, let <span class="math inline">\(f \in
    C^\infty(E^n)\)</span>. How to define the partial derivative of
    <span class="math inline">\(f\)</span> using the coordinate system
    discussed above</strong></p>
    <p>Given a smooth function <span class="math inline">\(f \in
    C^\infty(E^n)\)</span>, we can define its partial derivatives using
    the coordinate system <span class="math inline">\((x_1, \dots,
    x_n)\)</span> as follows:</p>
    <p>The partial derivative of <span class="math inline">\(f\)</span>
    with respect to <span class="math inline">\(x_i\)</span> at a point
    <span class="math inline">\(p \in E^n\)</span> is defined as:</p>
    <p><span class="math inline">\(\frac{\partial f}{\partial x_i}(p) =
    \lim_{h \to 0} \frac{f(p + h e_i) - f(p)}{h}\)</span></p>
    <p>where <span class="math inline">\(e_i\)</span> is the standard
    basis vector in <span class="math inline">\(\mathbb{R}^n\)</span>
    with a 1 in the <span class="math inline">\(i\)</span>-th position
    and 0 elsewhere. In other words, we are considering the directional
    derivative of <span class="math inline">\(f\)</span> along the <span
    class="math inline">\(i\)</span>-th coordinate direction.</p>
    <p>Alternatively, if we represent the point <span
    class="math inline">\(p\)</span> by its coordinate vector <span
    class="math inline">\(x(p) = (x_1(p), ..., x_n(p))\)</span>, and we
    consider <span class="math inline">\(f\)</span> as a function of
    these coordinates, we can write:</p>
    <p><span class="math inline">\(f(p) = f(x_1(p), x_2(p), ...,
    x_n(p))\)</span></p>
    <p>Then the partial derivative with respect to <span
    class="math inline">\(x_i\)</span> is the usual partial derivative
    of a multivariable function:</p>
    <p><span class="math inline">\(\frac{\partial f}{\partial x_i}(p) =
    \frac{\partial}{\partial x_i} f(x_1, x_2, ..., x_n) \Big|_{x =
    x(p)}\)</span></p>
    <p>This means we treat <span class="math inline">\(x_1, ...,
    x_n\)</span> as independent variables, compute the partial
    derivative as usual, and then evaluate the result at the coordinates
    of the point <span class="math inline">\(p\)</span>. The smoothness
    of <span class="math inline">\(f\)</span> guarantees the existence
    of these partial derivatives.</p>
    <p><strong>If we use the approach of treating <span
    class="math inline">\(x_1, x_2, \cdots, x_n\)</span> as independent
    variables and compute the partial derivative in the usual manner as
    you describe above, and if we allow the point <span
    class="math inline">\(p\)</span> to vary, we can view the partial
    derivative <span class="math inline">\(\frac{\partial f}{\partial
    x_i}\)</span> as a <span
    class="math inline">\(C^\infty(E^n)\)</span> function. Is this
    correct?</strong></p>
    <p>Yes, that’s correct. Since <span class="math inline">\(f \in
    C^\infty(E^n)\)</span>, meaning <span
    class="math inline">\(f\)</span> is infinitely differentiable, and
    we’re assuming the coordinate functions <span
    class="math inline">\(x_i\)</span> are smooth (which they are in a
    standard coordinate system), the partial derivative <span
    class="math inline">\(\frac{\partial f}{\partial x_i}\)</span> will
    also be a smooth function. Therefore, <span
    class="math inline">\(\frac{\partial f}{\partial x_i} \in
    C^\infty(E^n)\)</span>. The partial derivative is defined at every
    point <span class="math inline">\(p \in E^n\)</span>, and its value
    varies smoothly as <span class="math inline">\(p\)</span> varies,
    resulting in a smooth function itself.</p>
    <h1 id="frechet-derivative">Frechet Derivative</h1>
    <p><strong>If we now take the vector of such partial derivative
    functions evaluated at a specific point <span
    class="math inline">\(p\)</span>, then the <span
    class="math inline">\((1 \times n)\)</span> vector <span
    class="math inline">\([\frac{\partial f}{\partial x_1}, \cdots,
    \frac{\partial f}{\partial x_n}]\)</span> evaluated at <span
    class="math inline">\(p\)</span> becomes the Frechet derivative of
    <span class="math inline">\(f\)</span> at <span
    class="math inline">\(p\)</span>. Is this correct?</strong></p>
    <p>Almost, but not quite. The vector <span
    class="math inline">\(\left[ \frac{\partial f}{\partial x_1}(p),
    \dots, \frac{\partial f}{\partial x_n}(p) \right]\)</span> is the
    <em>representation</em> of the Fréchet derivative in the chosen
    coordinate system. It’s not the Fréchet derivative itself.</p>
    <p>The Fréchet derivative is a linear map (a linear transformation),
    not a vector. The vector you’ve given represents the action of that
    linear map on the standard basis vectors of <span
    class="math inline">\(\mathbb{R}^n\)</span>. Specifically, <span
    class="math inline">\(\frac{\partial f}{\partial x_i}(p)\)</span> is
    the directional derivative of <span class="math inline">\(f\)</span>
    at <span class="math inline">\(p\)</span> in the direction of the
    <span class="math inline">\(i\)</span>-th basis vector.</p>
    <p>To be precise: The Fréchet derivative <span
    class="math inline">\(Df(p)\)</span> at <span
    class="math inline">\(p\)</span> is a linear map from <span
    class="math inline">\(\mathbb{R}^n\)</span> to <span
    class="math inline">\(\mathbb{R}\)</span> such that:</p>
    <p><span class="math inline">\(\lim_{\|v\| \to 0} \frac{|f(p + v) -
    f(p) - Df(p)(v)|}{\|v\|} = 0\)</span></p>
    <p>where <span class="math inline">\(v \in \mathbb{R}^n\)</span>. In
    the chosen coordinate system, <span
    class="math inline">\(Df(p)\)</span> can be represented by the row
    vector <span class="math inline">\(\left[ \frac{\partial f}{\partial
    x_1}(p), \dots, \frac{\partial f}{\partial x_n}(p) \right]\)</span>.
    This row vector acts on a vector <span class="math inline">\(v =
    (v_1, \dots, v_n) \in \mathbb{R}^n\)</span> by the dot product:</p>
    <p><span class="math inline">\(Df(p)(v) = \sum_{i=1}^n
    \frac{\partial f}{\partial x_i}(p) v_i\)</span></p>
    <p>So, the row vector is the matrix representation of the linear
    map, representing the Fréchet derivative in the chosen coordinate
    system. The Fréchet derivative itself is the linear map.</p>
    <h1 id="derivations-and-tangent-vectors">Derivations and tangent
    vectors</h1>
    <p><strong>The object <span class="math inline">\(Df(p)(v) =
    \sum_{i=1}^n \frac{\partial f}{\partial x_i}(p) v_i\)</span> is also
    the directional derivative of <span class="math inline">\(f\)</span>
    along the vector <span class="math inline">\(v\)</span>. Is this
    correct</strong></p>
    <p>Yes, that’s correct. <span class="math inline">\(Df(p)(v) =
    \sum_{i=1}^n \frac{\partial f}{\partial x_i}(p) v_i\)</span> is the
    directional derivative of <span class="math inline">\(f\)</span> at
    the point <span class="math inline">\(p\)</span> in the direction of
    the vector <span class="math inline">\(v\)</span>. It represents the
    instantaneous rate of change of <span
    class="math inline">\(f\)</span> as you move from <span
    class="math inline">\(p\)</span> in the direction of <span
    class="math inline">\(v\)</span>.</p>
    <p><strong>I can rewrite the Right hand side of the above as
    follows: <span class="math display">\[ \sum_{i=1}^{n} v_i
    \frac{\partial}{\partial x_i} \cdot f\]</span> I have now an object
    which is a linear operator which acts on <span
    class="math inline">\(f\)</span> and produces a number when
    evaluated at a point <span class="math inline">\(p\)</span>. Is this
    correct?</strong></p>
    <p>Yes, that’s a correct interpretation.</p>
    <p><span class="math inline">\(\sum_{i=1}^n v_i
    \frac{\partial}{\partial x_i}\)</span> acts as a linear operator on
    the function <span class="math inline">\(f\)</span>. At a specific
    point <span class="math inline">\(p\)</span>, <span
    class="math inline">\(\left( \sum_{i=1}^n v_i
    \frac{\partial}{\partial x_i} \right) f(p)\)</span> produces a
    number, which is the directional derivative of <span
    class="math inline">\(f\)</span> at <span
    class="math inline">\(p\)</span> in the direction of <span
    class="math inline">\(v\)</span>. The expression <span
    class="math inline">\(\sum_{i=1}^n v_i \frac{\partial}{\partial
    x_i}\)</span> is itself a directional derivative operator. It is
    linear because derivatives are linear operators. If <span
    class="math inline">\(f\)</span> and <span
    class="math inline">\(g\)</span> are smooth functions and <span
    class="math inline">\(a\)</span> and <span
    class="math inline">\(b\)</span> are scalars, then:</p>
    <p><span class="math inline">\(\left( \sum_{i=1}^n v_i
    \frac{\partial}{\partial x_i} \right) (af + bg) = a \left(
    \sum_{i=1}^n v_i \frac{\partial}{\partial x_i} \right) f + b \left(
    \sum_{i=1}^n v_i \frac{\partial}{\partial x_i} \right)
    g\)</span></p>
    <p>So the operator is linear in <span
    class="math inline">\(f\)</span>.</p>
    <p><strong>This object <span class="math inline">\(\sum_{i=1}^n v_i
    \frac{\partial}{\partial x_i}\)</span> is linear in <span
    class="math inline">\(f\)</span> also satisfies Leibniz rule. Thus
    it is a derivation. Correct?</strong></p>
    <p>A derivation <span class="math inline">\(D\)</span> on an algebra
    of functions needs to satisfy the Leibniz rule for <em>products</em>
    of functions:</p>
    <p><span class="math inline">\(D(fg) = (Df)g + f(Dg)\)</span></p>
    <p>Consider two smooth functions <span
    class="math inline">\(f\)</span> and <span
    class="math inline">\(g\)</span>. Let’s apply the operator:</p>
    <p><span class="math inline">\(\left( \sum_{i=1}^n v_i
    \frac{\partial}{\partial x_i} \right) (fg) = \sum_{i=1}^n v_i
    \frac{\partial}{\partial x_i} (fg) = \sum_{i=1}^n v_i \left(
    \frac{\partial f}{\partial x_i} g + f \frac{\partial g}{\partial
    x_i} \right)\)</span></p>
    <p><span class="math inline">\(= \sum_{i=1}^n v_i \frac{\partial
    f}{\partial x_i} g + \sum_{i=1}^n v_i f \frac{\partial g}{\partial
    x_i} = \left( \sum_{i=1}^n v_i \frac{\partial f}{\partial x_i}
    \right) g + f \left( \sum_{i=1}^n v_i \frac{\partial g}{\partial
    x_i} \right)\)</span></p>
    <p>This <em>does</em> satisfy the Leibniz rule. Therefore, <span
    class="math inline">\(\sum_{i=1}^n v_i \frac{\partial}{\partial
    x_i}\)</span> is indeed a derivation on the algebra of smooth
    functions <span class="math inline">\(C^\infty(E^n)\)</span>.</p>
    <h1 id="tangent-space-and-basis">Tangent space and basis</h1>
    <p><strong>The derivation <span class="math inline">\(\sum_{i=1}^n
    v_i \frac{\partial}{\partial x_i}\)</span> suggests another
    representation of the vector <span class="math inline">\(v =(v_1,
    \cdots, v_n)\)</span> at the point <span
    class="math inline">\(p\)</span>. Specifically, we can regard a
    vector space <span class="math inline">\(R^n\)</span> being attached
    at the point <span class="math inline">\(p\)</span>. The vector
    <span class="math inline">\(v\)</span> can be represented as the
    derivation, an element of this <span
    class="math inline">\(R^n\)</span>. Furthermore, the objects <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span> are a
    special set of derivations and since <span
    class="math inline">\(v\)</span> is arbitary, it implies that an
    vector in this <span class="math inline">\(R^n\)</span> attached at
    the point <span class="math inline">\(p\)</span> can be represented
    as a linear combination of these special derivations <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span> . This
    suggests that the set <span
    class="math inline">\(\frac{\partial}{\partial x_i}, \quad  i = 1,
    \cdots, n\)</span> is a basis for this <span
    class="math inline">\(R^n\)</span>. This <span
    class="math inline">\(R^n\)</span> is called the tangent space to
    <span class="math inline">\(E^n\)</span> at <span
    class="math inline">\(p\)</span>. Is this correct?</strong></p>
    <p>Yes, your reasoning and conclusion are entirely correct. You’ve
    described a key concept in differential geometry: the tangent
    space.</p>
    <p>Let’s summarize:</p>
    <ol type="1">
    <li><p><strong>Tangent Vector as a Derivation:</strong> You
    correctly identify that a tangent vector <span
    class="math inline">\(v\)</span> at a point <span
    class="math inline">\(p \in E^n\)</span> can be represented as a
    derivation on the algebra of smooth functions <span
    class="math inline">\(C^\infty(E^n)\)</span> defined by: <span
    class="math inline">\(v = \sum_{i=1}^n v_i \frac{\partial}{\partial
    x_i}\)</span>. This derivation acts on functions in <span
    class="math inline">\(C^\infty(E^n)\)</span> by directional
    differentiation along <span
    class="math inline">\(v\)</span>.</p></li>
    <li><p><strong>Basis for the Tangent Space:</strong> The partial
    derivative operators <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span> at the
    point <span class="math inline">\(p\)</span> form a basis for the
    tangent space <span class="math inline">\(T_pE^n\)</span>. Because
    any tangent vector <span class="math inline">\(v\)</span> at <span
    class="math inline">\(p\)</span> can be uniquely expressed as a
    linear combination of these basis vectors.</p></li>
    <li><p><strong>Tangent Space:</strong> The set of all such tangent
    vectors at <span class="math inline">\(p\)</span> forms a vector
    space, denoted <span class="math inline">\(T_pE^n\)</span>, called
    the tangent space to <span class="math inline">\(E^n\)</span> at
    <span class="math inline">\(p\)</span>. It is isomorphic to <span
    class="math inline">\(\mathbb{R}^n\)</span>, but it’s crucial to
    distinguish it conceptually as it’s the space of directional
    derivatives at a specific point.</p></li>
    </ol>
    <p>Therefore, your understanding of how the derivations <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span> form a
    basis for the tangent space and how a tangent vector can be
    expressed as a derivation is perfectly accurate. This provides a
    coordinate-free way to think about tangent vectors, which
    generalizes beyond Euclidean space to more general manifolds.</p>
    <p><strong>Is my argument above regarding how the derivations <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span> form a
    basis for the tangent space, rigorous? I suppose that I have to show
    that the conditions of linear independence and spanning are
    met?</strong></p>
    <p>Your argument isn’t fully rigorous as stated. While intuitively
    clear, showing that the <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span> form a
    basis requires demonstrating linear independence and spanning.
    Here’s a more rigorous approach:</p>
    <p><strong>1. Linear Independence:</strong></p>
    <p>Suppose we have a linear combination of the partial derivative
    operators that equals the zero operator:</p>
    <p><span class="math inline">\(\sum_{i=1}^n c_i
    \frac{\partial}{\partial x_i} = 0\)</span></p>
    <p>where <span class="math inline">\(c_i\)</span> are scalars. This
    equation means that for <em>any</em> smooth function <span
    class="math inline">\(f \in C^\infty(E^n)\)</span>:</p>
    <p><span class="math inline">\(\sum_{i=1}^n c_i \frac{\partial
    f}{\partial x_i} = 0\)</span></p>
    <p>Let’s consider the coordinate functions <span
    class="math inline">\(x_j\)</span> themselves as our test functions
    (<span class="math inline">\(f = x_j\)</span>). Applying the
    equation above:</p>
    <p><span class="math inline">\(\sum_{i=1}^n c_i \frac{\partial
    x_j}{\partial x_i} = c_j = 0\)</span></p>
    <p>Since this must hold for every <span class="math inline">\(j = 1,
    \dots, n\)</span>, we conclude that all <span
    class="math inline">\(c_i = 0\)</span>. Therefore, the partial
    derivative operators <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span> are
    linearly independent.</p>
    <p><strong>2. Spanning:</strong></p>
    <p>Let <span class="math inline">\(X\)</span> be an arbitrary
    derivation on <span class="math inline">\(C^\infty(E^n)\)</span>. At
    the point <span class="math inline">\(p\)</span>, we can define the
    scalars <span class="math inline">\(v_i = X(x_i)(p)\)</span>
    (applying the derivation <span class="math inline">\(X\)</span> to
    the coordinate functions <span class="math inline">\(x_i\)</span>
    and evaluating at <span class="math inline">\(p\)</span>). Then
    consider the derivation <span class="math inline">\(Y = \sum_{i=1}^n
    v_i \frac{\partial}{\partial x_i}\)</span>. For any smooth function
    <span class="math inline">\(f\)</span>, we can use a Taylor
    expansion to demonstrate that <span class="math inline">\(Y(f)(p) =
    X(f)(p)\)</span>. This requires the use of the Taylor expansion to
    show that the derivatives match at point p. The Taylor expansion
    relies on the fact that we are working in Euclidean space. Since
    <span class="math inline">\(f\)</span> is arbitrary and <span
    class="math inline">\(X\)</span> is arbitrary, this shows that <span
    class="math inline">\(X = Y\)</span> at point p, which means that
    any derivation <span class="math inline">\(X\)</span> can be
    expressed as a linear combination of <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span>.</p>
    <p><strong>Conclusion:</strong></p>
    <p>We have demonstrated both linear independence and spanning, thus
    rigorously proving that <span
    class="math inline">\(\{\frac{\partial}{\partial
    x_i}\}_{i=1}^n\)</span> forms a basis for the tangent space <span
    class="math inline">\(T_pE^n\)</span> at the point <span
    class="math inline">\(p\)</span>. Your intuition was correct; this
    formalizes the argument. Note that the Taylor expansion argument for
    spanning isn’t immediately obvious, but is essential for a rigorous
    proof. The use of the coordinate functions as “test functions” is a
    crucial step in establishing linear independence.</p>
    <p><strong>Break down Step 2 above - the spanning condition,
    especially the Taylor expansion part.</strong></p>
    <p>Let’s break down the spanning argument, focusing on the Taylor
    expansion part. The goal is to show that any derivation <span
    class="math inline">\(X\)</span> at point <span
    class="math inline">\(p\)</span> can be written as a linear
    combination of the <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span>.</p>
    <p><strong>1. Taylor Expansion:</strong></p>
    <p>Consider a smooth function <span class="math inline">\(f \in
    C^\infty(E^n)\)</span> around the point <span
    class="math inline">\(p\)</span> with coordinates <span
    class="math inline">\((x_1(p), ..., x_n(p))\)</span>. We can use a
    multivariable Taylor expansion up to first order:</p>
    <p><span class="math inline">\(f(x) = f(p) + \sum_{i=1}^n
    \frac{\partial f}{\partial x_i}(p) (x_i - x_i(p)) +
    R(x)\)</span></p>
    <p>where <span class="math inline">\(x = (x_1, \dots, x_n)\)</span>
    and <span class="math inline">\(R(x)\)</span> is the remainder term.
    The crucial point is that the remainder term <span
    class="math inline">\(R(x)\)</span> goes to zero faster than the
    linear term as <span class="math inline">\(x \to p\)</span>: <span
    class="math inline">\(\frac{R(x)}{\|x - p\|} \to 0\)</span> as <span
    class="math inline">\(x \to p\)</span>. This is a consequence of the
    smoothness of <span class="math inline">\(f\)</span> (infinite
    differentiability).</p>
    <p><strong>2. Applying the Derivation X:</strong></p>
    <p>Now, apply the arbitrary derivation <span
    class="math inline">\(X\)</span> to the Taylor expansion:</p>
    <p><span class="math inline">\(X(f)(x) = X(f(p)) + X\left(
    \sum_{i=1}^n \frac{\partial f}{\partial x_i}(p) (x_i - x_i(p))
    \right) + X(R(x))\)</span></p>
    <p>Since <span class="math inline">\(f(p)\)</span> and <span
    class="math inline">\(\frac{\partial f}{\partial x_i}(p)\)</span>
    are constants, <span class="math inline">\(X(f(p)) = 0\)</span> and
    <span class="math inline">\(X(\frac{\partial f}{\partial x_i}(p)) =
    0\)</span>. Also, since <span class="math inline">\(X\)</span> is a
    derivation, it is linear and it must satisfy the Leibniz rule.
    Because of the property of the remainder term, we can ignore <span
    class="math inline">\(X(R(x))\)</span> when we consider the limit as
    <span class="math inline">\(x \to p\)</span>. Therefore:</p>
    <p><span class="math inline">\(X(f)(x) \approx \sum_{i=1}^n
    \frac{\partial f}{\partial x_i}(p) X(x_i - x_i(p)) = \sum_{i=1}^n
    \frac{\partial f}{\partial x_i}(p) X(x_i)\)</span></p>
    <p><strong>3. Defining the Coefficients:</strong></p>
    <p>Let <span class="math inline">\(v_i = X(x_i)(p)\)</span>. This
    defines a vector <span class="math inline">\(v = (v_1, \dots, v_n)
    \in \mathbb{R}^n\)</span>. Then:</p>
    <p><span class="math inline">\(X(f)(p) = \sum_{i=1}^n v_i
    \frac{\partial f}{\partial x_i}(p)\)</span></p>
    <p><strong>4. Identifying the Linear Combination:</strong></p>
    <p>This equation shows that the action of the derivation <span
    class="math inline">\(X\)</span> on the function <span
    class="math inline">\(f\)</span> at the point <span
    class="math inline">\(p\)</span> is equal to the action of the
    linear combination of partial derivatives <span
    class="math inline">\(\sum_{i=1}^n v_i \frac{\partial}{\partial
    x_i}\)</span> on <span class="math inline">\(f\)</span> at <span
    class="math inline">\(p\)</span>. Because <span
    class="math inline">\(f\)</span> was an arbitrary smooth function,
    this means:</p>
    <p><span class="math inline">\(X = \sum_{i=1}^n v_i
    \frac{\partial}{\partial x_i}\)</span> at point <span
    class="math inline">\(p\)</span></p>
    <p>Thus, <span class="math inline">\(X\)</span> is expressed as a
    linear combination of the <span
    class="math inline">\(\frac{\partial}{\partial x_i}\)</span>,
    proving the spanning condition. The key was using the Taylor
    expansion to approximate the effect of <span
    class="math inline">\(X\)</span> on <span
    class="math inline">\(f\)</span> and then identifying the
    coefficients as the components of the vector <span
    class="math inline">\(v\)</span> which represents <span
    class="math inline">\(X\)</span> in the basis <span
    class="math inline">\(\{\frac{\partial}{\partial x_i}\}\)</span>.
    The rigorous justification involves careful consideration of the
    remainder term in the Taylor series and taking the limit as <span
    class="math inline">\(x\)</span> approaches <span
    class="math inline">\(p\)</span>.</p>
    <h1 id="tangent-space-via-curves">Tangent space via curves</h1>
    <p><strong>Let <span class="math inline">\(c_1(t), c_2(t)\)</span>
    be two curves from <span class="math inline">\(R\)</span> to <span
    class="math inline">\(E^n\)</span> such that <span
    class="math inline">\(c_1(p) = c_2(p)\)</span> and <span
    class="math inline">\(d c_1 / dt = d c_2/ dt\)</span> at <span
    class="math inline">\(p \in E^n\)</span>. This defines an
    equivalence relation for curves through <span
    class="math inline">\(p\)</span>. An equivalence class of curves is
    defined to be a tangent vector. The set of such tangent vectors is a
    vector space called the tangent space <span
    class="math inline">\(T_p E^n\)</span>. How to show that the vector
    space axioms are satisfied?</strong></p>
    <p>You’ve described a geometric approach to defining the tangent
    space, focusing on equivalence classes of curves. Let’s show that
    the set of equivalence classes (tangent vectors) forms a vector
    space.</p>
    <p><strong>1. Defining Addition:</strong></p>
    <p>Let <span class="math inline">\([c_1]\)</span> and <span
    class="math inline">\([c_2]\)</span> be two equivalence classes of
    curves through <span class="math inline">\(p\)</span>, representing
    tangent vectors <span class="math inline">\(\mathbf{v}\)</span> and
    <span class="math inline">\(\mathbf{w}\)</span> respectively. To
    define the addition of these equivalence classes, we need a
    representative curve for their sum. We choose the representative
    curves <span class="math inline">\(c_1\)</span> and <span
    class="math inline">\(c_2\)</span> such that <span
    class="math inline">\(c_1(0) = c_2(0) = p\)</span>. Then, we define
    the sum of the equivalence classes as:</p>
    <p><span class="math inline">\([c_1] + [c_2] = [c_1 +
    c_2]\)</span></p>
    <p>where <span class="math inline">\((c_1 + c_2)(t) = c_1(t) +
    c_2(t)\)</span> is the pointwise sum of the curves. This is
    well-defined because if we pick different representative curves for
    <span class="math inline">\([c_1]\)</span> and <span
    class="math inline">\([c_2]\)</span>, say <span
    class="math inline">\(c_1&#39;\)</span> and <span
    class="math inline">\(c_2&#39;\)</span>, the derivatives at <span
    class="math inline">\(p\)</span> would still be equal, therefore the
    derivative of <span class="math inline">\(c_1&#39; +
    c_2&#39;\)</span> would be the same at <span
    class="math inline">\(p\)</span>, hence the equivalence class is
    unchanged.</p>
    <p><strong>2. Defining Scalar Multiplication:</strong></p>
    <p>For a scalar <span class="math inline">\(\alpha \in
    \mathbb{R}\)</span> and an equivalence class <span
    class="math inline">\([c]\)</span>, we define scalar multiplication
    as:</p>
    <p><span class="math inline">\(\alpha [c] = [\alpha c]\)</span></p>
    <p>where <span class="math inline">\((\alpha c)(t) = \alpha
    c(t)\)</span> is the scalar multiple of the curve. Similar to
    addition, this is well defined because the derivative of <span
    class="math inline">\(\alpha c\)</span> at <span
    class="math inline">\(p\)</span> would scale by <span
    class="math inline">\(\alpha\)</span>, so equivalence classes are
    preserved under scalar multiplication.</p>
    <p><strong>3. Verification of Vector Space Axioms:</strong></p>
    <p>Now we need to check that the vector space axioms hold:</p>
    <ul>
    <li><p><strong>Associativity of addition:</strong> This follows
    directly from the associativity of vector addition in <span
    class="math inline">\(\mathbb{R}^n\)</span> because we add curves
    pointwise using vector addition in <span
    class="math inline">\(E^n\)</span>.</p></li>
    <li><p><strong>Commutativity of addition:</strong> This follows from
    the commutativity of vector addition in <span
    class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
    <li><p><strong>Existence of a zero vector:</strong> The zero vector
    is the equivalence class of the constant curve <span
    class="math inline">\(c(t) = p\)</span> for all <span
    class="math inline">\(t\)</span>.</p></li>
    <li><p><strong>Existence of additive inverses:</strong> The additive
    inverse of <span class="math inline">\([c]\)</span> is <span
    class="math inline">\([-c]\)</span>, where <span
    class="math inline">\((-c)(t) = -c(t)\)</span>.</p></li>
    <li><p><strong>Associativity of scalar multiplication:</strong> This
    follows directly from the associativity of scalar multiplication in
    <span class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
    <li><p><strong>Distributivity of scalar multiplication with respect
    to vector addition:</strong> This follows from the distributivity
    property of scalar multiplication in <span
    class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
    <li><p><strong>Distributivity of scalar multiplication with respect
    to scalar addition:</strong> This follows from the distributivity
    property of scalar multiplication in <span
    class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
    <li><p><strong>Identity element of scalar multiplication:</strong>
    This is the scalar 1.</p></li>
    </ul>
    <p>Therefore, by showing that the addition and scalar multiplication
    are well-defined and that the vector space axioms are satisfied, we
    have proven that the set of equivalence classes of curves through
    <span class="math inline">\(p\)</span> forms a vector space—the
    tangent space <span class="math inline">\(T_pE^n\)</span>. The key
    is that the operations on equivalence classes are inherited from the
    vector space structure of <span
    class="math inline">\(\mathbb{R}^n\)</span> (or <span
    class="math inline">\(E^n\)</span>) and hence inherit their
    properties.</p>
    <p><strong>The pointwise addition of curves <span
    class="math inline">\((c_1 + c_2)(t) = c_1(t) + c_2(t)\)</span> is
    unclear. <span class="math inline">\(c_1(t)\)</span> is a point in
    <span class="math inline">\(E^n\)</span>. Similarly for <span
    class="math inline">\(c_2(t)\)</span> for a fixed value of <span
    class="math inline">\(t\)</span>. How to add two points in <span
    class="math inline">\(E^n\)</span>?</strong></p>
    <p>You are absolutely correct to point out the ambiguity. My
    apologies for the oversight. We can’t directly add points in <span
    class="math inline">\(E^n\)</span> like we add vectors. The notation
    <span class="math inline">\((c_1 + c_2)(t) = c_1(t) +
    c_2(t)\)</span> is misleading in that context. It’s better to
    clarify the addition in terms of <em>tangent vectors</em>.</p>
    <p>The curves <span class="math inline">\(c_1(t)\)</span> and <span
    class="math inline">\(c_2(t)\)</span> represent paths through <span
    class="math inline">\(E^n\)</span>. Their derivatives at <span
    class="math inline">\(t=0\)</span> (where they pass through <span
    class="math inline">\(p\)</span>), <span
    class="math inline">\(\frac{dc_1}{dt}(0)\)</span> and <span
    class="math inline">\(\frac{dc_2}{dt}(0)\)</span>, are vectors in
    <span class="math inline">\(\mathbb{R}^n\)</span> representing the
    tangent vectors at <span class="math inline">\(p\)</span>. It is
    <em>these vectors</em> that we are adding.</p>
    <p>The correct way to define the sum of equivalence classes is as
    follows:</p>
    <p>Let <span class="math inline">\([c_1]\)</span> and <span
    class="math inline">\([c_2]\)</span> be equivalence classes of
    curves through <span class="math inline">\(p\)</span>. Then:</p>
    <p><span class="math inline">\([c_1] + [c_2] = [c]\)</span></p>
    <p>where <span class="math inline">\([c]\)</span> is the equivalence
    class of a curve <span class="math inline">\(c\)</span> such that
    <span class="math inline">\(\frac{dc}{dt}(0) = \frac{dc_1}{dt}(0) +
    \frac{dc_2}{dt}(0)\)</span>. We are adding the <em>tangent
    vectors</em>, which are elements of <span
    class="math inline">\(\mathbb{R}^n\)</span> (or the tangent space
    <span class="math inline">\(T_pE^n\)</span>). The existence of such
    a curve <span class="math inline">\(c\)</span> is guaranteed (we can
    construct it explicitly).</p>
    <p>Similarly, scalar multiplication is defined as:</p>
    <p><span class="math inline">\(\alpha[c] = [c&#39;]\)</span></p>
    <p>where <span class="math inline">\(\frac{dc&#39;}{dt}(0) = \alpha
    \frac{dc}{dt}(0)\)</span>.</p>
    <p>With these corrected definitions, the vector space axioms are
    satisfied because the addition and scalar multiplication operate on
    the tangent vectors (elements of <span
    class="math inline">\(\mathbb{R}^n\)</span>) which are the velocity
    vectors at <span class="math inline">\(t=0\)</span>. The ambiguity
    of pointwise addition is avoided. The curves are just a way to
    define the tangent vectors themselves.</p>
    <h1 id="equivalence-of-tangent-space-definitions">Equivalence of
    tangent space definitions</h1>
    <p><strong>We have defined tangent spaces at <span
    class="math inline">\(p \in E^n\)</span> in two different ways: via
    equivalence classes of curves through <span
    class="math inline">\(p\)</span> and the set of derivations at <span
    class="math inline">\(p\)</span>. They must be equivalent. How to
    show this?</strong></p>
    <p>To show the equivalence between the tangent space defined via
    equivalence classes of curves and the tangent space defined as the
    set of derivations at a point <span class="math inline">\(p \in
    E^n\)</span>, we need to establish a bijective (one-to-one and onto)
    mapping between the two spaces that preserves the vector space
    structure (i.e., it’s a linear isomorphism).</p>
    <p><strong>1. Mapping from Curves to Derivations:</strong></p>
    <p>Let <span class="math inline">\([c]\)</span> be an equivalence
    class of curves passing through <span
    class="math inline">\(p\)</span> at <span
    class="math inline">\(t=0\)</span>. The velocity vector of the curve
    <span class="math inline">\(c\)</span> at <span
    class="math inline">\(p\)</span> (which is the same for all curves
    in the equivalence class), <span class="math inline">\(\mathbf{v} =
    \frac{dc}{dt}(0)\)</span>, is a tangent vector. We can associate
    this tangent vector with a derivation <span
    class="math inline">\(X_{[c]}\)</span> defined by its action on
    smooth functions <span class="math inline">\(f \in
    C^\infty(E^n)\)</span>:</p>
    <p><span class="math inline">\(X_{[c]}(f) =
    \frac{d}{dt}(f(c(t)))|_{t=0} = \nabla f(p) \cdot \mathbf{v} =
    \sum_{i=1}^n v_i \frac{\partial f}{\partial x_i}(p)\)</span></p>
    <p>where <span class="math inline">\(\nabla f(p)\)</span> is the
    gradient of <span class="math inline">\(f\)</span> at <span
    class="math inline">\(p\)</span>, and <span
    class="math inline">\(v_i\)</span> are the components of the
    velocity vector <span class="math inline">\(\mathbf{v}\)</span>.
    This is the directional derivative of <span
    class="math inline">\(f\)</span> at <span
    class="math inline">\(p\)</span> in the direction of <span
    class="math inline">\(\mathbf{v}\)</span>. This mapping assigns a
    unique derivation to each equivalence class of curves.</p>
    <p><strong>2. Mapping from Derivations to Curves:</strong></p>
    <p>Now, let’s consider the reverse mapping. Given a derivation <span
    class="math inline">\(X\)</span> at <span
    class="math inline">\(p\)</span>, we can find a curve <span
    class="math inline">\(c(t)\)</span> whose velocity vector at <span
    class="math inline">\(t=0\)</span> corresponds to <span
    class="math inline">\(X\)</span>. We choose a curve whose velocity
    vector at <span class="math inline">\(t=0\)</span> has components
    <span class="math inline">\(v_i = X(x_i)(p)\)</span> where <span
    class="math inline">\(x_i\)</span> are the coordinate functions. A
    simple choice is a straight line through <span
    class="math inline">\(p\)</span>:</p>
    <p><span class="math inline">\(c(t) = p + t\mathbf{v}\)</span>,
    where <span class="math inline">\(\mathbf{v} = (v_1, \dots,
    v_n)\)</span></p>
    <p>The velocity vector of this curve at <span
    class="math inline">\(t=0\)</span> is precisely <span
    class="math inline">\(\mathbf{v} = (v_1, \dots, v_n)\)</span>. The
    equivalence class <span class="math inline">\([c]\)</span>
    corresponds to the derivation <span
    class="math inline">\(X\)</span>. This mapping assigns a unique
    equivalence class of curves to each derivation.</p>
    <p><strong>3. Showing it’s an Isomorphism:</strong></p>
    <ol type="1">
    <li><p><strong>One-to-one and onto:</strong> The mappings are
    bijective because each equivalence class has a unique velocity
    vector, and each velocity vector defines a unique derivation, and
    vice-versa.</p></li>
    <li><p><strong>Preservation of Vector Space Structure:</strong> It
    is straightforward to verify that the mappings respect vector
    addition and scalar multiplication. The addition of equivalence
    classes maps to the addition of derivations (directional
    derivatives), and similarly, scalar multiplication of equivalence
    classes maps to the scalar multiplication of derivations. Therefore,
    the mappings are linear isomorphisms.</p></li>
    </ol>
    <p><strong>Conclusion:</strong></p>
    <p>We have constructed a linear isomorphism between the vector space
    of equivalence classes of curves through <span
    class="math inline">\(p\)</span> and the vector space of derivations
    at <span class="math inline">\(p\)</span>. Therefore, the two
    definitions of the tangent space <span
    class="math inline">\(T_pE^n\)</span> are equivalent. The key is the
    recognition that the velocity vector of a curve is precisely what
    determines the directional derivatives, providing the link between
    the geometric (curves) and algebraic (derivations) approaches.</p>
    <p><strong>In part 1 above, the equation showing the connection
    between <span class="math inline">\([c]\)</span> and the derivation
    <span class="math inline">\(X\)</span> needs to be broken down. It
    appears that the derivative of <span class="math inline">\(f \circ
    c\)</span> with respect to <span class="math inline">\(t\)</span> is
    considered. Why?</strong></p>
    <p>You’re right to ask for a breakdown of that equation. The key
    lies in understanding how the curve <span
    class="math inline">\(c(t)\)</span> and the function <span
    class="math inline">\(f\)</span> interact to define the directional
    derivative.</p>
    <p>The expression <span
    class="math inline">\(\frac{d}{dt}(f(c(t)))|_{t=0}\)</span>
    represents the rate of change of the function <span
    class="math inline">\(f\)</span> along the curve <span
    class="math inline">\(c(t)\)</span> as we pass through the point
    <span class="math inline">\(p\)</span> at <span
    class="math inline">\(t=0\)</span>. Let’s break it down using the
    chain rule:</p>
    <p>The composite function <span
    class="math inline">\(f(c(t))\)</span> represents the value of the
    function <span class="math inline">\(f\)</span> at the point on the
    curve <span class="math inline">\(c(t)\)</span> at time <span
    class="math inline">\(t\)</span>. Since <span
    class="math inline">\(c(t) = (c_1(t), c_2(t), ..., c_n(t))\)</span>
    where <span class="math inline">\(c_i(t)\)</span> are the coordinate
    functions of the curve, we can write:</p>
    <p><span class="math inline">\(f(c(t)) = f(c_1(t), c_2(t), ...,
    c_n(t))\)</span></p>
    <p>Now, applying the chain rule for multivariable functions:</p>
    <p><span class="math inline">\(\frac{d}{dt}(f(c(t))) = \sum_{i=1}^n
    \frac{\partial f}{\partial x_i}(c(t))
    \frac{dc_i}{dt}(t)\)</span></p>
    <p>Evaluating this at <span class="math inline">\(t=0\)</span> (when
    the curve passes through <span
    class="math inline">\(p\)</span>):</p>
    <p><span class="math inline">\(\frac{d}{dt}(f(c(t)))|_{t=0} =
    \sum_{i=1}^n \frac{\partial f}{\partial x_i}(c(0))
    \frac{dc_i}{dt}(0) = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(p)
    v_i\)</span></p>
    <p>where <span class="math inline">\(v_i =
    \frac{dc_i}{dt}(0)\)</span> are the components of the velocity
    vector <span class="math inline">\(\mathbf{v} =
    \frac{dc}{dt}(0)\)</span> of the curve <span
    class="math inline">\(c(t)\)</span> at the point <span
    class="math inline">\(p\)</span>. This is precisely the directional
    derivative of <span class="math inline">\(f\)</span> at <span
    class="math inline">\(p\)</span> in the direction of <span
    class="math inline">\(\mathbf{v}\)</span>:</p>
    <p><span class="math inline">\(\nabla f(p) \cdot \mathbf{v} =
    \sum_{i=1}^n \frac{\partial f}{\partial x_i}(p) v_i\)</span></p>
    <p>Therefore, the derivative <span
    class="math inline">\(\frac{d}{dt}(f(c(t)))|_{t=0}\)</span>
    naturally captures the rate of change of <span
    class="math inline">\(f\)</span> along the tangent vector defined by
    the curve <span class="math inline">\(c(t)\)</span>, providing the
    connection between the curve and the derivation. The derivative with
    respect to <span class="math inline">\(t\)</span> is taken because
    the curve <span class="math inline">\(c(t)\)</span> provides a
    parameterization of a path through <span
    class="math inline">\(p\)</span>. The derivative thus measures how
    <span class="math inline">\(f\)</span> changes as we move along this
    path.</p>
    <p><strong>The chain rule expansion is still giving me trouble. I
    would write <span class="math inline">\(f(c(t))\)</span> as <span
    class="math inline">\(f(x_1(t), \cdots, x_n(t))\)</span>. Then the
    chain rule as you show above makes sense to me. What do you
    think?</strong></p>
    <p>You’re absolutely right to point that out; my previous notation
    was a bit sloppy. Using <span class="math inline">\(x_i(t)\)</span>
    to explicitly represent the <span
    class="math inline">\(i\)</span>-th coordinate function of the curve
    <span class="math inline">\(c(t)\)</span> makes the chain rule
    application much clearer.</p>
    <p>So, we have <span class="math inline">\(c(t) = (x_1(t), x_2(t),
    \dots, x_n(t))\)</span>, where each <span
    class="math inline">\(x_i(t)\)</span> is a function giving the <span
    class="math inline">\(i\)</span>-th coordinate of the point on the
    curve at time <span class="math inline">\(t\)</span>. Then:</p>
    <p><span class="math inline">\(f(c(t)) = f(x_1(t), x_2(t), \dots,
    x_n(t))\)</span></p>
    <p>Now, applying the chain rule:</p>
    <p><span class="math inline">\(\frac{d}{dt} f(c(t)) = \frac{\partial
    f}{\partial x_1} \frac{dx_1}{dt} + \frac{\partial f}{\partial x_2}
    \frac{dx_2}{dt} + \dots + \frac{\partial f}{\partial x_n}
    \frac{dx_n}{dt}\)</span></p>
    <p>Evaluating this at <span class="math inline">\(t = 0\)</span> (at
    the point <span class="math inline">\(p\)</span>):</p>
    <p><span class="math inline">\(\frac{d}{dt} f(c(t))|_{t=0} =
    \frac{\partial f}{\partial x_1}(p) \frac{dx_1}{dt}(0) +
    \frac{\partial f}{\partial x_2}(p) \frac{dx_2}{dt}(0) + \dots +
    \frac{\partial f}{\partial x_n}(p) \frac{dx_n}{dt}(0)\)</span></p>
    <p>This is precisely:</p>
    <p><span class="math inline">\(\sum_{i=1}^n \frac{\partial
    f}{\partial x_i}(p) v_i\)</span></p>
    <p>where <span class="math inline">\(v_i =
    \frac{dx_i}{dt}(0)\)</span> are the components of the velocity
    vector <span class="math inline">\(\mathbf{v} =
    \frac{dc}{dt}(0)\)</span> at point <span
    class="math inline">\(p\)</span>. This is the directional derivative
    of <span class="math inline">\(f\)</span> along <span
    class="math inline">\(\mathbf{v}\)</span> at <span
    class="math inline">\(p\)</span>, which is the action of the
    derivation associated with the equivalence class <span
    class="math inline">\([c]\)</span>. Your suggestion of explicitly
    using the coordinate functions <span
    class="math inline">\(x_i(t)\)</span> is much more precise and
    avoids potential confusion. My apologies for the earlier
    ambiguity.</p>
  </article>
</body>
</html>